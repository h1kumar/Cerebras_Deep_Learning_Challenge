{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(train_Y, val_Y, test_Y):\n",
    "    '''\n",
    "    Computing one hot encoding of output labels\n",
    "\n",
    "    train_1H_Y= Computing one hot encoding of training labels train_Y\n",
    "    val_1H_Y= Computing one hot encoding of validation labels val_Y\n",
    "    test_1H_Y= Computing one hot encoding of test labels test_Y\n",
    "    '''\n",
    "    ## One Hot Encoding Training Set\n",
    "    train_1H_Y = np.zeros((train_Y.shape[1], 10))\n",
    "    train_1H_Y[np.arange(train_Y.shape[1]), train_Y] = 1\n",
    "    train_1H_Y = train_1H_Y.T\n",
    "\n",
    "    ## One Hot Encoding Validation Set\n",
    "    val_1H_Y = np.zeros((val_Y.shape[1], 10))\n",
    "    val_1H_Y[np.arange(val_Y.shape[1]), val_Y] = 1\n",
    "    val_1H_Y = val_1H_Y.T\n",
    "\n",
    "    ## One Hot Encoding Test Set\n",
    "    tst_1H_Y = np.zeros((test_Y.shape[1], 10))\n",
    "    tst_1H_Y[np.arange(test_Y.shape[1]), test_Y] = 1\n",
    "    tst_1H_Y = tst_1H_Y.T\n",
    "    return train_1H_Y,val_1H_Y,tst_1H_Y\n",
    "\n",
    "def init_weights_2(layers):\n",
    "    '''\n",
    "    Random Weight Initialization of each layer of Neural Network\n",
    "    \n",
    "    layers= list containing no. of hidden units in each layer\n",
    "    \n",
    "    params= output that contains random weights and biases for each layer\n",
    "    ''' \n",
    "    params = {}\n",
    "    L = len(layers)\n",
    "    for l in range (1,L):\n",
    "        if l == 0:\n",
    "            params['W' + str(l)] = np.ones((layers[l],layers[l-1]))\n",
    "            params['b' + str(l)] = np.zeros((layers[l],1))\n",
    "        else:\n",
    "            params['W' + str(l)] = np.random.randn(layers[l],layers[l-1])*0.01\n",
    "            params['b' + str(l)] = np.zeros((layers[l],1))\n",
    "    return params\n",
    "\n",
    "def relu(A_prev,W,B):\n",
    "    '''\n",
    "    Computing Relu Activation for a hidden layer\n",
    "    \n",
    "    A_prev= output of previous layer and input into current layer\n",
    "    W = Weights of the current layer\n",
    "    B = Bias of the current layer\n",
    "    \n",
    "    A= Relu Activation output for the current layer\n",
    "    Cache= stores the inputs and the output computed\n",
    "    '''\n",
    "    Z = np.dot(W,A_prev) + B\n",
    "    A = np.maximum(0,Z)\n",
    "    linear_input = (A_prev,W,B)\n",
    "    activation_output = A\n",
    "    cache = (linear_input,activation_output)\n",
    "    return A,cache\n",
    "\n",
    "def sigmoid(A_prev,W,B,sigm = 0):\n",
    "    '''\n",
    "    Computing Sigmoid(s)/Tanh(x) Activation for a hidden layer\n",
    "\n",
    "    A_prev= output of previous layer and input into current layer\n",
    "    W = Weights of the current layer\n",
    "    B = Bias of the current layer\n",
    "    sigm = value 0 computes standard sigmoid and value 1 computes 1.7159tanh(2x/3)\n",
    "    \n",
    "    A= Sigmoid Activation output for the current layer\n",
    "    Cache= stores the inputs and the output computed\n",
    "    '''\n",
    "    Z = np.dot(W,A_prev) + B\n",
    "    if sigm == 0:\n",
    "        A = 1./(1+np.exp(-Z))\n",
    "    else:\n",
    "        A = 1.7159*np.tanh(2*Z/float(3))\n",
    "    linear_input = (A_prev,W,B)\n",
    "    activation_output = A\n",
    "    cache = (linear_input,activation_output)\n",
    "    return A,cache\n",
    "\n",
    "def softmax(A_prev,W,B):\n",
    "    '''\n",
    "    Computing Softmax Activation for a Output layer \n",
    "    \n",
    "    A_prev= output of previous layer and input into current layer\n",
    "    W = Weights of the current layer\n",
    "    B = Bias of the current layer\n",
    "    \n",
    "    A= Softmax Activation output for the current layer\n",
    "    Cache= stores the inputs and the output computed\n",
    "    '''\n",
    "    Z = np.dot(W,A_prev) + B\n",
    "    Nr = np.exp(Z)\n",
    "    Dr = np.sum(Nr, axis = 0, keepdims=True)\n",
    "    A = np.divide(Nr,Dr)\n",
    "    linear_input = (A_prev,W,B)\n",
    "    activation_output = A\n",
    "    cache = (linear_input,activation_output)\n",
    "    return A,cache\n",
    "\n",
    "def linear_prop_forward_2(X,parameters,sigm = 0):\n",
    "    ''' \n",
    "    Implementation of Linear Forward Propagation of the Neural Network \n",
    "    \n",
    "    X= Matrix of Input images data set\n",
    "    parameters= Hidden layer weights and bias of all the layers\n",
    "    sigm= 0: sigmoid activation, 1 : tanh(x) activation, 2: relu activation\n",
    "    \n",
    "    AL= output of softmax layer\n",
    "    caches= contains cache of all the hidden layers\n",
    "    '''\n",
    "    L = int(len(parameters)/2)\n",
    "    caches = []\n",
    "    for l in range(1,L):\n",
    "        if l == 1:\n",
    "            x_hat  = X - np.sum(X, axis = 1, keepdims=True)/X.shape[1]\n",
    "            A_prev = x_hat + parameters['b'+str(l)]\n",
    "            cache = ((X,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)]),A_prev)\n",
    "            caches.append(cache)\n",
    "        else:\n",
    "            if sigm == 0 or sigm == 1:\n",
    "                A,cache = sigmoid(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],sigm)\n",
    "            if sigm == 2:\n",
    "                A,cache = relu(A_prev,parameters['W'+str(l)],parameters['b'+str(l)])\n",
    "            caches.append(cache)\n",
    "    \n",
    "    AL,cache = softmax(A,parameters['W'+str(L)],parameters['b'+str(L)])\n",
    "    caches.append(cache)\n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(Y,AL):\n",
    "    ''' \n",
    "    Cross Entropy Cost Computation \n",
    "    \n",
    "    Y = Actual label of image data set\n",
    "    AL= Softmax layers output\n",
    "    \n",
    "    cost= cross entropy cost of the mini-batch\n",
    "    '''\n",
    "    m = AL.shape[1]\n",
    "    cost = -np.sum(np.multiply(Y,np.log(AL)))/m\n",
    "    return cost\n",
    "\n",
    "def relu_derivative(dA, activation_cache):\n",
    "    ''' \n",
    "    Relu Derivative Calculation for hidden layer \n",
    "    \n",
    "    dA= gradient of cost w.r.t to current layers output\n",
    "    activation_cache= output value of current layer computed during forward propagation\n",
    "    \n",
    "    dZ= derivative of cost w.r.t to current layer activation input\n",
    "    '''\n",
    "    relu_deriv = activation_cache>0\n",
    "    dZ = np.multiply(dA,relu_deriv)\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_derivative(dA, activation_cache,sigm = 0):\n",
    "    ''' \n",
    "    Sigmoid Derivative Calculation for hidden layer \n",
    "    \n",
    "    dA= gradient of cost w.r.t to current layers output\n",
    "    activation_cache= output value of current layer computed during forward propagation\n",
    "    sigm= 0: dZ calculation w.r.t to sigmoid, 1: dZ calculation w.r.t tanh(x) activation\n",
    "    \n",
    "    dZ= derivative of cost w.r.t to current layer activation input\n",
    "    '''\n",
    "    if sigm == 0:\n",
    "        dZ = np.multiply(dA,activation_cache*(1-activation_cache))\n",
    "    else:      \n",
    "        activation_cache = activation_cache/float(1.7159)\n",
    "        dZ = np.multiply(dA,(1 - np.multiply(activation_cache,activation_cache))*1.7159*2/float(3))\n",
    "    return dZ\n",
    "\n",
    "def grad_calculation(dZ, cache):\n",
    "    ''' \n",
    "    Gradient Calculation for hidden layer paramenters \n",
    "    \n",
    "    dZ=derivative of cost w.r.t to current layer activation input\n",
    "    cache= inputs computed during forward propagation\n",
    "    \n",
    "    dA_Prev= gradient of previous layers output\n",
    "    dW= gradient of weight for current layer\n",
    "    db= gradient of bias for current layer\n",
    "    '''\n",
    "    A_Prev, W, b = cache\n",
    "    m = A_Prev.shape[1]\n",
    "    dW = np.dot(dZ, A_Prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims=True)/m\n",
    "    dA_Prev = np.dot(W.T, dZ)\n",
    "    return (dA_Prev, dW, db)\n",
    "\n",
    "def linear_prop_backward_2(AL,Y,caches,sigm = 0):\n",
    "    ''' \n",
    "    Implementation of Linear Backward Propagation thereby computing gradients of all the hidden layer parameters \n",
    "    \n",
    "    AL= Outputs computed by Softmax Layer \n",
    "    Y= Actual one hot encoding of Image Data set\n",
    "    caches= linear and activation caches during forward propagation for all the hidden layers\n",
    "    sigm= 0: sigmoid derivative to be used, 1: tanh derivative to be used, 2: relu derivative to be used\n",
    "    \n",
    "    grad= gradients computed for all hidden layer parameters\n",
    "    '''\n",
    "    L = len(caches)\n",
    "    grad = {}\n",
    "    current_cache = caches[L-1]\n",
    "    linear_cache, activation_cache = current_cache    \n",
    "    grad[\"dA\"+str(L)],grad[\"dW\"+str(L)],grad[\"db\" + str(L)] = grad_calculation(Y-AL,linear_cache)\n",
    "    for i in reversed (range(L-1)):\n",
    "        current_cache = caches[i]\n",
    "        if i >= 1:\n",
    "            if sigm == 0 or sigm == 1:\n",
    "                dZ = sigmoid_derivative(grad[\"dA\"+str(i+2)], current_cache[1], sigm)\n",
    "            if sigm == 2:\n",
    "                dZ = relu_derivative(grad[\"dA\"+str(i+2)], current_cache[1])\n",
    "            grad[\"dA\"+str(i+1)],grad[\"dW\"+str(i+1)],grad[\"db\" + str(i+1)] = grad_calculation(dZ, current_cache[0])\n",
    "\n",
    "        else:\n",
    "            grad[\"db\" + str(i+1)] = np.sum(grad[\"dA\" + str(i+2)], axis = 1, keepdims=True)/grad[\"dA\" + str(i+2)].shape[1]\n",
    "    return grad\n",
    "\n",
    "def update_params_2(params, grad, lr):\n",
    "    ''' \n",
    "    Updating hidden layer parameters \n",
    "    \n",
    "    params= parameters of hidden layers\n",
    "    grad= gradients computed for all hidden layer parameters\n",
    "    lr = learning rate\n",
    "    \n",
    "    params= updated parameters of hidden layers\n",
    "    '''\n",
    "    L = int(len(params)/2)\n",
    "    for l in range(L):\n",
    "        if l == 0:\n",
    "            params[\"b\"+str(l+1)] = params[\"b\"+str(l+1)] + lr*grad[\"db\"+str(l+1)]\n",
    "        else:\n",
    "            params[\"W\"+str(l+1)] = params[\"W\"+str(l+1)] + lr*grad[\"dW\"+str(l+1)]\n",
    "            params[\"b\"+str(l+1)] = params[\"b\"+str(l+1)] + lr*grad[\"db\"+str(l+1)]\n",
    "    return params\n",
    "\n",
    "def predict_2(X,Y,parameters,sigm = 0):\n",
    "    ''' \n",
    "    Accuracy Prediction for input data set\n",
    "    \n",
    "    X= input matrix of images data set\n",
    "    Y= actual labels of input data set images\n",
    "    parameters= hidden layer parameters\n",
    "    sigm= 0: sigmoid activation, 1: tanh(x) activation, 2: relu activation, for hidden layers\n",
    "    \n",
    "    accuracy= percentage correct predicted by the neural network model\n",
    "    '''\n",
    "    AL,_ = linear_prop_forward_2(X,parameters,sigm)\n",
    "    ypred = np.argmax(AL, axis = 0).reshape(Y.shape[0],Y.shape[1])\n",
    "    count = 0\n",
    "    for i in range(Y.shape[1]):\n",
    "        if ypred[0,i] == Y[0,i]:\n",
    "            count = count + 1\n",
    "    accuracy = count/Y.shape[1]*100\n",
    "    return accuracy\n",
    "\n",
    "def get_mb(X, Y, mb_size = 64): \n",
    "    '''\n",
    "    Generating Mini Batches of a given mini-batch size \n",
    "    \n",
    "    X= Huge Training Data set of Input Images\n",
    "    Y= Actual Output labels of Input Images\n",
    "    mb_size= Mini batch size\n",
    "    \n",
    "    mini_batches= list of all minibatches generated of given mini batch size\n",
    "    '''\n",
    "    m = X.shape[1] \n",
    "    mini_batches = []\n",
    "    shuf_nums = list(np.random.permutation(m))\n",
    "    rand_X = X[:, shuf_nums]\n",
    "    rand_Y = Y[:, shuf_nums].reshape((10,m))\n",
    "    total_mb = int(m/mb_size)\n",
    "    for i in range(0, total_mb):\n",
    "        mb_X,mb_Y = rand_X[:,i*mb_size : (i+1)*mb_size], rand_Y[:,i*mb_size : (i+1)*mb_size]\n",
    "        mini_batch = (mb_X, mb_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mb_size != 0:\n",
    "        mb_X, mb_Y = rand_X[:,total_mb * mb_size:], rand_Y[:,total_mb * mb_size:]\n",
    "        mini_batch = (mb_X, mb_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "\n",
    "def multi_layer_model(X, Y_1H, Y, val_X, val_Y, test_X, test_Y, layers_dims, \n",
    "                      learning_rate=0.005, num_iterations=50, mini_batch_size = 64,sigm=0):\n",
    "    '''\n",
    "    Multi layer Neural Network that learns classification of digits and accurately predits their classes\n",
    "    \n",
    "    X= Input training set on which the Model traings\n",
    "    Y_1H= one hot encoded input of the corresponding input images\n",
    "    Y= Class labels of training images dataset\n",
    "    val_X= Validation images dataset\n",
    "    val_Y= Class labels of validation images dataset\n",
    "    test_X= Test images dataset\n",
    "    test_Y= Class labels of test images dataset\n",
    "    layer_dims= hidden units in each layer of the network\n",
    "    learning_rate= learning rate to be used for gradient update\n",
    "    num_iterations= number of epochs of the training set\n",
    "    mini_batch_size= size of the mini batches to be generated from input training set\n",
    "    sigm= activation function to be used for hidden layers, 0: sigmoid, 1: tanh(x), 2:relu\n",
    "    \n",
    "    parameters= weights and bias learnt after iterating through the trianing set\n",
    "    '''    \n",
    "    parameters = init_weights_2(layers_dims)\n",
    "    costs = []\n",
    "    train_acc = []\n",
    "    val_acc   = []\n",
    "    test_acc  = []\n",
    "    for i in range(num_iterations):\n",
    "        mini_batches = get_mb(X, Y_1H, mini_batch_size)\n",
    "        for m in mini_batches:\n",
    "            mini_X,mini_Y = m\n",
    "            AL,caches = linear_prop_forward_2(mini_X,parameters,sigm)\n",
    "            cost = compute_cost(mini_Y,AL)\n",
    "            grads = linear_prop_backward_2(AL,mini_Y,caches,sigm)\n",
    "            parameters = update_params_2(parameters,grads,learning_rate)\n",
    "            costs.append(cost)\n",
    "\n",
    "        train_acc.append(predict_2(X,     Y,       parameters, sigm))\n",
    "        val_acc.append(predict_2(val_X,   val_Y,   parameters, sigm))\n",
    "        test_acc.append(predict_2(test_X, test_Y,  parameters, sigm))\n",
    "        \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('Training Cost')\n",
    "    plt.xlabel('#Mini Batch Iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(train_acc, label= \"Train Acc\")\n",
    "    plt.plot(val_acc, label= \"Val Acc\")\n",
    "    plt.plot(test_acc, label= \"Test Acc\")\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('#Epoch')\n",
    "    plt.legend(loc = 4, fontsize = 'medium')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oil_spill_model_extra_credit(path = \"\"):\n",
    "    '''\n",
    "    Reading Training and Test Data Sets, Normalizing them by dividing by 225,\n",
    "    Splitting Training set into Training and Validation Set\n",
    "    \n",
    "    path= path of directory containing the training and test datasets\n",
    "\n",
    "    train_X, train_Y= Reading Training Input Data in train_X and their actual labels in train_Y\n",
    "    val_X, val_Y= Contains Validation Set Input Data in val_X and the corresponding labels in val_Y\n",
    "    test_X, testY= Contains Test Input Data in test_X and Test labels in test_Y\n",
    "    \n",
    "    acc_train= Accuracy on Training Model\n",
    "    acc_val= Accuracy on Validation Model\n",
    "    acc_test= Accuracy on Test Model\n",
    "    '''\n",
    "    train_X, train_Y = MNIST(path, gz = True).load_training()\n",
    "    test_X,  test_Y  = MNIST(path, gz = True).load_testing()\n",
    "\n",
    "    train_X = (np.array(train_X).T)/255\n",
    "    train_Y = np.array(train_Y)\n",
    "    train_Y = train_Y.reshape(1,60000)\n",
    "\n",
    "    val_X   = train_X[:,50000:60000]\n",
    "    train_X = train_X[:,:50000]\n",
    "\n",
    "    val_Y   = train_Y[:,50000:60000]\n",
    "    train_Y = train_Y[:,:50000]\n",
    "\n",
    "    test_X = (np.array(test_X).T)/255\n",
    "    test_Y = np.array(test_Y).reshape(1,10000)\n",
    "\n",
    "    print (\"Shape of Training Data set\", train_X.shape, train_Y.shape)\n",
    "    print (\"Shape of Validation Data set\", val_X.shape,   val_Y.shape)\n",
    "    print (\"Shape of Test Data set\", test_X.shape,  test_Y.shape)\n",
    "\n",
    "    ## Required parameters\n",
    "    activation = 2\n",
    "    layers_dims = [784,784,64,10]\n",
    "    train_1H_Y,val_1H_Y,tst_1H_Y = one_hot_encoding(train_Y,val_Y,test_Y)\n",
    "\n",
    "    ## Running the Mlti Layer Neural Network Model\n",
    "    parameters = multi_layer_model(train_X, train_1H_Y, train_Y, val_X, val_Y, test_X, test_Y,\n",
    "                                   layers_dims, num_iterations=220, learning_rate=0.04, \n",
    "                                   mini_batch_size = 128, sigm=activation)\n",
    "\n",
    "    ## Predicting Accuracy of Trained Model\n",
    "    acc_train = predict_2(train_X, train_Y, parameters, sigm = activation)\n",
    "    acc_val   = predict_2(val_X,   val_Y,   parameters, sigm = activation)\n",
    "    acc_test  = predict_2(test_X,  test_Y,  parameters, sigm = activation)\n",
    "    print (\"Training Accuracy = \", acc_train)\n",
    "    print (\"Validation Accuracy = \", acc_val)\n",
    "    print (\"Test Accuracy = \", acc_test)\n",
    "    \n",
    "    return acc_train,acc_val,acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data set (784, 50000) (1, 50000)\n",
      "Shape of Validation Data set (784, 10000) (1, 10000)\n",
      "Shape of Test Data set (784, 10000) (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "acc_train, acc_val, acc_test = oil_spill_model_extra_credit(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
